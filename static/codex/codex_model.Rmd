---
title: "Codex model"
output:
  html_document:
    number_sections: false
    self_contained: yes
    code_folding: none
    df_print: paged
    css: tables.css
author: "Mark Webster"
date: 2020-03-29
---

```{r, "setup", include=FALSE}
library(data.table)
library(stringr)
library(DT)
library(dplyr)
library(ggplot2)
library(rstan)
library(codex)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE)
```

# Simulation configuration

```{r, "check ess"}
untidy_results <- readRDS("vs_split.rds")
parm_info <- as.data.table(summary(untidy_results)$summary, keep.rownames = "parameter")[parameter != "lp_", 
                                                                                         c("parameter", "n_eff")]
```

Current model sample size is 4000. However, since we use a Markov Chain sampler, the samples are correlated: the effective sample size varies between different parameters, within the range (`r floor(min(parm_info$n_eff))`, `r ceiling(max(parm_info$n_eff))`).

# Posterior model performance

```{r, "read model results"}
results <- readRDS("tidy_vs_split.rds")
```

```{r, "read model samples"}
sim <- results$tidy_results
```

```{r, "load data", message=FALSE}
normal_matches <- all_matches[recorder == "charnel_mouse" &
                                (is.element(victory, c("normal", "")) | is.na(victory)) &
                                format == "forum"]
mean_data <- prepare_match_data_for_modelling(normal_matches, starters, nicknames, mean = TRUE)
attach(mean_data)
starter_names <- starters
spec_names <- specs
starters <- codex::starters
```

```{r, "Matchup predictions for mean performance models"}
won_effect <- 2*rep(w, each = nrow(sim$matchup)) - 1
won <- w == 1
match_names <- paste0(normal_matches$tournament,
                      " Round ", normal_matches$round, 
                      ", Game ", normal_matches$round_match_number,
                      ": ", normal_matches$player1, " ", normal_matches$deck1,
                      " vs. ",
                      normal_matches$player2, " ", normal_matches$deck2, 
                      ", won by ", normal_matches$victor)
match_res <- colMeans(1/(1 + exp(-won_effect*sim$matchup[, 1:M, drop = FALSE])))
matchups_models <- data.table(`match name` = match_names,
                              `result probability` = match_res,
                              fairness = 1 - 2*abs(match_res - 1/2))
formatRound(datatable(matchups_models, rownames = FALSE, filter = "top",
                      options = list(dom = "tip"),
                      caption = "Recorded matches with model's expected probability of observed result. Fairness rates the fairness of the matchup, including players, between 0 and 1."), 
            2:3, 2)
```

```{r, "Density plots for predictions for matches with simple decks"}
ggplot(matchups_models, aes(.data$`result probability`)) +
  geom_density() +
  xlab("result probability") +
  ggtitle("Post-hoc probability for given match results")
ggplot(matchups_models, aes(.data$`result probability`)) +
  geom_density() +
  xlab("result probability") +
  ggtitle("Post-hoc probability for given match results, by victor turn") +
  facet_wrap(. ~ factor(ifelse(won, "P1 win", "P2 win")))
matchups_models[, .(`result probability`, fairness,
                    P1_win_prob = ifelse(won, 
                                         `result probability`, 
                                         1 - `result probability`),
                    rounded = round(ifelse(won, 
                                           `result probability`, 
                                           1 - `result probability`),
                                    1),
                    win = won)
                ][, .(rate = ifelse(.N == 0, NA, mean(win)),
                      prob = ifelse(.N == 0, NA, mean(P1_win_prob))),
                  by = c("rounded")] %>% 
  ggplot(aes(.data$prob, .data$rate)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  xlab("mean forecast p1 win probability") + 
  ylab("observed p1 win rate") + 
  ggtitle("P1 win rate compared to predicted win probability grouped to nearest 10%")
```

If the model predicted matches by saying the most likely winner would win, its performance on the above matches, which were used to fit it, would be `r sum(matchups_models[["result probability"]] > 0.5)` out of `r nrow(matchups_models)` (`r round(mean(matchups_models[["result probability"]] > 0.5)*100, 1)`%). Since it gives a probability of each player winning, we can use proper scoring rules instead (the closer to zero, the better):

```{r, "model scores"}
model_log_scores <- matchups_models[, .(`log score` = -log(`result probability`), 
                                        `Brier score` = (1 - `result probability`)^2)
                                    ][, .(model = c("predict every match as 5-5", "current model"), 
                                          `log score` = c(log(2), mean(`log score`)), 
                                          `Brier score` = c(1/4, mean(`Brier score`)))]
kable(model_log_scores, digits = 3)
```

# Player skill

Player skills are given as their (additive) effect on their log-odds of winning a match. Skill is currently assumed to not change over time, so given skill levels for long-absent players are narrowly-distributed compared to how certain we'd really be about their current skill level. It'll also favour players who were veterans before the earliest recorded match, because the period where they learned the ropes is not included in their match records.

```{r, "extract vs split models player skills"}
vs_sim_skills <- melt(as.data.table(sim$player), id.vars = character(0), 
                      variable.name = "player", value.name = "player skill")[, .(`player skill`, mean = mean(`player skill`)), 
                                                                             by = "player"]
vs_split_player_levels <- vs_sim_skills[order(mean), unique(player)]

vs_skill <- vs_sim_skills[, .(player, `player skill`, mean, model = "Versus model")]
vs_skill_by_forum_skill <- vs_skill[, .(player = factor(player, vs_split_player_levels), `player skill`, mean, model)]
```

```{r, "plot vs models player skills", fig.height=10, warning=FALSE}
plot_player_skill(vs_skill_by_forum_skill[is.element(player, players)], "overall", waiver()) +
  ylim(-5, 5)
```

```{r, "plot probability player is best", fig.height=10}
prob_largest_skill <- get_probability_largest(sim$player)
plot_probability_largest(prob_largest_skill) +
  xlab("player") +
  ylab("probability best player")
```

```{r, "player skill table"}
formatRound(datatable(merge(vs_sim_skills[, .(mean = mean[1]), by = "player"
                                          ][, .(player = as.character(player), `mean skill` = mean)],
                            as.data.table(prob_largest_skill)[, .(player = effect, `prob. best (n = 4000)` = `effect size`)],
                            on = "player", sort = FALSE),
                      rownames = FALSE, filter = "top", options = list(dom = "tip")),
            2, 3)
```

# Opposed component effects

Decks are treated in opposing pairs: P1 starter versus P2 starter, P1 starter versus each P2 spec, and so on. Each match has 16 such pairs. Each pair's effect is given as its additive effect on the log-odds of a player 1 victory. The component's effects are added to given overall matchup between the decks, before accounting for player skill levels.

Note that these pair effects are *not* direct appraisals of how the components fare against each other. For each, the Green vs. Black effect doesn't assess how those two decks decks match up against each other, it assesses how decks using those starter decks *tend* to match up against each other. Similarly, Blood vs. Future doesn't, directly assess how those two specs compete at, say, Tech II, because I don't record tech building choices. Instead, it shows how P1 decks including Blood tend to fare against P2 decks including Future. Note that this also ignores interactions between different pairs completely.

To examine the matchup between two particular decks, add their components in the relevant Deck components column. The overall matchup is then given below the table, as both the log-odds and the probability of a player 1 victory. Individual pair effects are given in the displayed table rows.

Currently I've not added the players in the same table to account for skill effects. In the meantime, since player skill tends to have a larger effect than the deck matchup, don't compare the deck matchups to your own match outcomes too strictly, unless you can manually add the effects from the player table (remember to subtract the P2 effect, not add it).

```{r, "extract vs model matrices"}
vs_array <- extract_vs_model_array(results)
```

```{r, "simple versus table"}
vs_split_table <- as.data.table(vs_array)[, .(sample = V1, 
                                              `P1 component` = factor(V2, c(starter_names, spec_names)),
                                              `P2 component` = factor(V3, c(starter_names, spec_names)),
                                              value)
                                          ][, .(mean = mean(value)),
                                            by = c("P1 component", "P2 component")
                                            ][order(`P1 component`, `P2 component`)]
vs_sketch <- htmltools::withTags(table(class = "display",
                                       thead(tr(th(colspan = 2, "Deck components"),
                                                th(rowspan = 2, "Mean Player 1 win log-odds effect")),
                                             tr(lapply(c("Player 1", "Player 2"), th))),
                                       tfoot(tr(th("Player 1"), th("Player 2"), th(0)))))
vs_split_JS_code <- "function(row, data, start, end, display) {
  var api = this.api(), data;
  total_mean = api.column(2, {search: 'applied'}).data().reduce( function(a, b) { return a + b}, 0);
  $( api.column(2).footer() ).html('Total: ' + total_mean.toFixed(2) + '; P1 win prob: ' + (100/(1+Math.exp(-total_mean))).toFixed(1) + '%')
  }"
datatable(vs_split_table, rownames = FALSE, filter = "top", container = vs_sketch, 
          options = list(dom = "ftip", pageLength = 16, footerCallback = JS(vs_split_JS_code))) %>% 
  formatRound(3, digits = 3)
```

# Monocolour matchups

Since we're most interested in whether monocolour decks are reasonably balanced, here are matchup results for the monocolour decks. The three black vertical lines in each plot facet show the matchup quartiles.

```{r, "extract monodecks"}
mononames <- sort(paste0("Mono", setdiff(starters$starter, "Neutral")))
monobits <- prepare_deck_names_for_modelling(mononames, starters, nicknames)
```

```{r, "extract vs split model monocolour matchups"}
vs_monos <- get_matchups(results, mononames)
```

```{r, make vs model monocolour matchup plots}
plot_matchup_samples(vs_monos)
```

```{r, "print vs model monocolour matchups in order"}
vs_monos_print <- vs_monos[, .(`P1 win probability` = mean(prob_matchup)), 
                           by = c("P1", "P2")
                           ][, .(P1, P2, `P1 win probability`,
                                 matchup = paste(format(round(`P1 win probability`, 2)*10, nsmall = 1),
                                                 format(10 - round(`P1 win probability`, 2)*10, nsmall = 1),
                                                 sep = "-"),
                                 fairness = 1 - 2*abs(`P1 win probability` - 1/2))]
formatRound(datatable(vs_monos_print, rownames = FALSE, filter = "top",
                      options = list(dom = "ftip")), 
            c(3, 5), c(3, 2))
```

# Model variances

Each type of component in the model has a different variance in the effect; inference for the variances is also done in the model simulation. The below plot shows the variances for each component type, scaled by how many such components go into a matchup, i.e. two player skill components, one starter vs. starter component, six starter vs. spec / spec vs. starter components, and nine spec vs. spec components.

```{r, "plot versus model variances"}
variance_info <- get_variances(sim)
plot_variances(variance_info)
```

On average, total player skill effects on match outcome are about `r round(variance_info[type == "player/deck ratio", mean(value)], 2)` as variable as total deck effects. This is a rough measure of how important to a match the players are, compared to the decks.

# Nash equilibria

For a set of decks, we can take the matchup samples, determine the Nash equilibrium for each sample, and take each deck's mean weight over the equilibria to find the general Nash equilibrium. This takes proper account of the model's uncertainty over the matchups. Using it to choose a deck is equivalent to Thompson sampling: for player skills, this would randomly choose a player to state as the best, weighted by the players' probabilities of being the best. This approach is better calibrated.

Nash equilibria are given for three types of player: `P1` when known to be going first, `P2` when known to be going second, and `Both` for the more common case where the deck is chosen before a coin flip to determine who goes first.

## Monocolour

```{r, "probabilistic versus model equilibrium"}
vs_mono_array <- get_matchup_array(results, mononames)
mono_stoch_nash <- get_nash_equilibria(vs_mono_array)
mono_mean_nash <- apply(mono_stoch_nash, 1:2, mean)
calibrated_mono_nash <- as.data.table(mono_mean_nash, keep.rownames = "Player")[, c(.(Player = factor(Player, 
                                                                                                      c("P1", "P2", "Both"))), 
                                                                                    .SD), 
                                                                                .SDcols = dimnames(mono_stoch_nash)[[2]]]
```

```{r, "calibrated monocolour Nash win"}
kable(print_nash(calibrated_mono_nash), digits = 3)
```

The chance of winning before determining play order is 50%, as we'd expect if we've calculated things properly.

```{r, "calibrated monocolour Nash details"}
kable(reformat_used_nash(calibrated_mono_nash), digits = 3)
```

Black is dominant, as expected given that it's known as a strong faction. Other decks have a larger presence when it's known whether you're going first or second. However, there is a lot of uncertainty: even Blue, considered weak due to an especially lopsided matchup against Black, hasn't been cut.

## Multicolour

```{r, "get used normal deck Nash equilibria"}
multicolour_stoch_nash <- readRDS("multicolour_nash_vs_split.rds")
```

### Win probability

```{r, "calibrated possibly normal Nash datatable win probabilities"}
kable(print_nash(multicolour_stoch_nash), digits = 3L)
```

The first player seems to be more advantaged than in the monocolour game, though not by much.

### Non-zero deck weights

```{r, "calibrated possibly normal Nash datatable"}
DT_nash(multicolour_stoch_nash, 4L, options = list(dom = "tip"))
```

`r if(ncol(multicolour_stoch_nash) == 2L + 3084) "Notably, no decks have been eliminated, so there is still a lot of uncertainty."`

# Counters

We could also suppose that we want the best counter to a known opposing deck, with unknown turn order, among all legal multicolour decks. Averaging over the matchup samples again, we find how likely each deck is to be the best counter-pick to each other deck.

The full counter table is too large to show here, so here are some highlights. We get the following top counter-picks (i.e. highest probability of being best counter) for each opponent (there are a few ties):

```{r, "multicolour counter-picks for lognormal prior"}
counters <- readRDS("multicolour_counters.rds")
formatRound(datatable(counters[, .SD[`Probability best counter` == max(`Probability best counter`)],
                               by = "Opponent"
                               ][order(-`Probability best counter`)],
                      filter = "top", rownames = FALSE,
                      options = list(dom = "ftip")),
            3:4, 4L)
```

Here is each deck's counter with highest mean win probability:

```{r, "multicolour highest-probability counter-picks for lognormal prior"}
formatRound(datatable(counters[, .SD[`Counter win probability` == max(`Counter win probability`)],
                               by = "Opponent"
                               ][order(-`Counter win probability`)],
                      filter = "top", rownames = FALSE,
                      options = list(dom = "ftip")),
            3:4, 4L)
```

Here are the top counter-picks for a few notable decks. First, Nightmare:

```{r, "deck countering example"}
knitr::kable(head(counter("Nightmare", vs_array)),
             digits = c(NA, nchar(1/dim(vs_array)[1]) - 2L, 3))
```

Miracle Grow:

```{r, "Miracle Grow counters"}
knitr::kable(head(counter("Miracle Grow", vs_array)),
             digits = c(NA, nchar(1/dim(vs_array)[1]) - 2L, 3))
```

[Past]/Anarchy/Peace:

```{r, "PPA counters"}
knitr::kable(head(counter("[Past]/Anarchy/Peace", vs_array)),
             digits = c(NA, nchar(1/dim(vs_array)[1]) - 2L, 3))
```

[Discipline/Strength]/Finesse:

```{r, "Daymare counters"}
knitr::kable(head(counter("[Discipline/Strength]/Finesse", vs_array)),
             digits = c(NA, nchar(1/dim(vs_array)[1]) - 2L, 3))
```

[Demonology]/Anarchy/Balance:

```{r, "DAB counters"}
knitr::kable(head(counter("[Demonology/Anarchy/Balance", vs_array)),
             digits = c(NA, nchar(1/dim(vs_array)[1]) - 2L, 3))
```

```{r, "best Nash deck"}
best <- reformat_used_nash(multicolour_stoch_nash)[1, Deck]
```

Here are the counters for `r best`, the deck with highest weight in the Nash mean:

```{r, "best's counter"}
knitr::kable(head(counter(best, vs_array)),
             digits = c(NA, nchar(1/dim(vs_array)[1]) - 2L, 3))
```

```{r, "hardest deck to counter"}
hardest <- counters[, .SD[`Probability best counter` == max(`Probability best counter`)
                          ][`Counter win probability` == max(`Counter win probability`)],
                    by = "Opponent"
                    ][`Counter win probability` == min(`Counter win probability`)]
```

Here are the counters for `r hardest$Opponent`, the deck whose most-likely-best counter has the lowest mean win probability (`r format(hardest[["Counter win probability"]], digits = 3L)`):

```{r, "counters for hardest to counter"}
knitr::kable(head(counter(as.character(hardest$Opponent), vs_array)),
             digits = c(NA, nchar(1/dim(vs_array)[1]) - 2L, 3))
```

Finally, here's the full counter table for the monocolour decks:

```{r, "counters for monocolour decks"}
monocounters <- rbindlist(lapply(setNames(mononames, mononames), counter, vs_array, "mono"),
                          idcol = "Opponent")
formatRound(datatable(monocounters, filter = "top", rownames = FALSE,
                      options = list(dom = "ftip")),
            3:4, 3L)
```

Black is the most-likely-best counter to everyone, except for Red (White) and White (Purple).
