---
title: "R has no consistent table class"
author: ~
date: '2024-01-08'
slug: []
categories: []
tags:
- Programming
- R
- Relational model
- autodb
subtitle: ''
summary: ''
authors: []
lastmod: '2024-01-08'
featured: no
draft: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
math: true
---

## The usual case

R, in addition to being array-based, can also be table-based: it has a table class in the base language, `data.frame`. This is great, because a lot of data comes in table form.

Here are some simple examples:

```{r}
x <- data.frame(
  a = rep(1:3, 4),
  b = rep(1:2, 6)
)
x
y <- data.frame(
  a = rep(1, 5)
)
y
```

One thing we can do with these tables is to look for, or remove, duplicate rows:

```{r}
duplicated(x)
duplicated(y)
unique(x)
unique(y)
```

Simple enough, right? Right.

## The edge case

Now let's try this with a data frame with no columns. This is something that R allows, so this should work as expected.

```{r}
z <- y[, FALSE, drop = FALSE]
z
```

Now, what do we expect to happen when we look for duplicates? Well, every row is the same, so every row after the first is a duplicate, so `unique` should leave a single row. The fact that the rows contain no data is irrelevant. What actually happens?

```{r}
duplicated(z)
```

`duplicated` returns a zero-length vector, as if there were no rows. This results in R claiming there are no rows after removing duplicates:

```{r}
unique(z)
```

Uh oh. How about if we only have the one row to begin with?

```{r}
w <- z[1, , drop = FALSE]
w
duplicated(w)
unique(w)
```

Oh, dear.

## Why this matters

In practice, a table with no columns is not going to turn up much, so you could argue that this doesn't matter. However, it should matter, if nothing else, for reasons of consistency: if we're working programmatically, we have no idea what dimension of table we're working with.

In fact, I've run into this problem multiple times when writing the `autodb` package for decomposing a data table into a partially-normalised database.

A database is composed of several relations, which are tables with some additional information. One piece of additional information is the relation's (candidate) keys, which are sets of the columns that, together, uniquely determine the rows. Each row has a unique set of values for the key's columns; vice versa, knowing the values for the key's columns determines which row we're looking at.

When turning a table of real data into a database, you can get a relation with an empty key. This happens when a column has the same value in every row: its value is constant, and determinable with no information. Such a relation can only have 0 or 1 rows, since an empty key can't distinguish between multiple rows.

There are a few reasons an empty key is a problem in R, given how we saw its data frames deal with this case, but let's take the example where we're checking that a given database is valid. One thing we need to check is that the columns in each key of a relation have unique values over its rows.

For example, suppose `x` above has both of its columns as its sole key. Does the key have unique values over its rows? No, because there are duplicates:

```{r}
x_key <- c("a", "b")
anyDuplicated((x[, x_key, drop = FALSE])) # returns 0 if unique
```

However, removing the duplicates makes the key values unique:

```{r}
anyDuplicated(unique(x)[, x_key, drop = FALSE])
```

Now, let's try validating a valid table with an empty key, which can only have 0 or 1 rows:

```{r}
v <- data.frame(a = 1L, b = 2L, c = FALSE)
v
v_key <- character()
anyDuplicated(v[, v_key, drop = FALSE]) # the right answer...
duplicated(v[, v_key, drop = FALSE]) # ... for the wrong reason
```

How about if that table invalidly has multiple rows?

```{r}
u <- data.frame(a = c(1L, 2L), b = c(2L, 3L), c = c(FALSE, TRUE))
u
u_key <- character()
anyDuplicated(u[, u_key, drop = FALSE]) # the wrong answer...
duplicated(u[, u_key, drop = FALSE]) # ... for the wrong reason
```

This shows that we can run into this problem, even when dealing with realistic data.
This is clearly a problem when writing a library that models databases! I end up having to write nasty code like this:

```{r}
dups <- if (length(u_key) == 0) {
  if (nrow(u) == 0)
    logical() # length 0 boolean vector
  else
    c(FALSE, rep(TRUE, nrow(u) - 1))
}else
  duplicated(u[, u_key, drop = FALSE])
dups
u[dups, , drop = FALSE]
```

## Third-party table classes are inconsistent too

OK, R's base `data.frame` class is inconsistent, but people also like to use the `tibble` and `data.table` classes instead, from their eponymous libraries. Do they do any better?

Here's `tibble`:

```{r}
library(tibble)
z_tib <- as_tibble(z) # should be 5x0
z_tib
w_tib <- as_tibble(w) # should be 1x0
w_tib
duplicated(z_tib)
try(unique(z_tib))
duplicated(w_tib)
try(unique(w_tib))
```

That's no good. How about `data.table`?

```{r}
library(data.table)
z_dt <- as.data.table(z) # should be 5x0
z_dt
w_dt <- as.data.table(w) # should be 5x0
w_dt
duplicated(z_dt)
unique(z_dt)
duplicated(w_dt)
unique(w_dt)
```

As much as I like `data.table` over `tibble`, this is even worse: the rows are all dropped on conversion. Creating the table directly as a `data.table`, instead of converting from a `data.frame`, makes no difference.

These are the main three table classes, but there are a few others. I looked at a few used for large data storage, but assessing most of them requires writing and reading to a file, rather than directly converting from a data frame. I'll go over this in the next section.

The one I found that does take direct conversion is `arrow`, which is an interface for Apache's Arrow C++ library. How does `arrow` do?

```{r}
library(arrow)
z_arw <- as_arrow_table(z)
z_arw
w_arw <- as_arrow_table(w)
w_arw
try(duplicated(z_arw))
unique(z_arw)
try(duplicated(w_arw))
unique(w_arw)
```

Not well.[^1]

[^1]: That message about `duplicated` only applying to vectors is a consequence of there being no `duplicated` method for Arrow tables.

## Files and file-driven table classes aren't consistent either

We'll look at three file-driven classes here: `feather` and `fst` from their eponymous packages, and `parquet`. Since `parquet` should be able to read from several file formats, we'll check this one as we go.[^2] We also end up looking briefly at `vroom`, due to some error messages from `parquet`.

[^2]: Feather and Parquet files can also be read with the `arrow` package, with the same result, since both are integrated with Apache Arrow.

The basic issue is that writing a zero-column data frame to a CSV file results in something that can't be parsed properly:

```{r}
tf <- tempfile()
write.csv(z, tf, row.names = FALSE)
readLines(tf)
try(read.csv(tf, row.names = FALSE))
```

`parquet` and `vroom` don't have much better luck writing and reading it:

```{r}
tf_parquet <- tempfile() # for writing parquet files via parquetize
tf_parquet_arrow <- tempfile() # for writing parquet files via arrow
try(parquetize::csv_to_parquet(tf, path_to_parquet = tf_parquet))
vroom::vroom(tf, delim = ",")
```

I wasn't expecting any option to turn a $5 \times 0$ table into a $0 \times 1$ table, but there it is.

Writing the row names doesn't improve matters much:

```{r}
tf_rn <- tempfile()
write.csv(z, tf_rn, row.names = TRUE)
readLines(tf_rn)
try(read.csv(tf_rn, row.names = TRUE))
try(parquetize::csv_to_parquet(tf_rn, path_to_parquet = tf_parquet))
vroom::vroom(tf_rn, delim = ",")
```

`vroom` now returns a $5 \times 1$ table, where the row names are misread as the single column's values.

How about `feather`?

```{r}
tf_feather <- tempfile()
tf_feather_arrow <- tempfile()
feather::write_feather(z, tf_feather)
feather::read_feather(tf_feather)
arrow::write_feather(z, tf_feather_arrow)
arrow::read_feather(tf_feather_arrow)
try(feather::read_feather(tf_feather_arrow))
arrow::read_feather(tf_feather)
```

We have slightly better luck here, depending on which package we use to handle Feather files.

We're not so lucky with `fst`:

```{r}
tf_fst <- tempfile()
fst::write_fst(z, tf_fst)
fst::read_fst(tf_fst)
arrow::write_parquet(z, tf_parquet_arrow)
arrow::read_parquet(tf_parquet_arrow)
```

Let's summarise everything done about for the $5 \times 0$ table `z`:

```{r, echo=FALSE, results='hide'}
dim_summary <- data.frame(
  format = c(
    "vroom w/o rownames",
    "vroom w/ rownames",
    "feather (write feather, read feather)",
    "feather (write arrow, read arrow)",
    "feather (write feather, read arrow)",
    "fst",
    "parquet"
  ),
  rows = vapply(
    list(
      vroom::vroom(tf, delim = ",", show_col_types = FALSE, .name_repair = "unique_quiet"),
      vroom::vroom(tf_rn, delim = ",", show_col_types = FALSE, .name_repair = "unique_quiet"),
      feather::read_feather(tf_feather),
      arrow::read_feather(tf_feather_arrow),
      arrow::read_feather(tf_feather),
      fst::read_fst(tf_fst),
      arrow::read_parquet(tf_parquet_arrow)
    ),
    nrow,
    integer(1)
  ),
  cols = vapply(
    list(
      vroom::vroom(tf, delim = ",", show_col_types = FALSE, .name_repair = "unique_quiet"),
      vroom::vroom(tf_rn, delim = ",", show_col_types = FALSE, .name_repair = "unique_quiet"),
      feather::read_feather(tf_feather),
      arrow::read_feather(tf_feather_arrow),
      arrow::read_feather(tf_feather),
      fst::read_fst(tf_fst),
      arrow::read_parquet(tf_parquet_arrow)
    ),
    ncol,
    integer(1)
  )
)
```

```{r, echo=FALSE}
knitr::kable(dim_summary)
```

The only library that gives the correct dimensions here is `feather`. However, its file-read result is a `tibble`, and we know they don't handle zero-column tables properly, so things like checking for duplicates still won't work properly. Additionally, it's only correct when writing the file using the original `feather` package: that package hasn't been updated since 2019, since the format was integrated into Apache Arrow, so the more current version gets it wrong.

## Pandas is no better

Come on, we can't make it look like Python is preferable.

```{python}
import pandas as pd
```

A 2x1 table works as expected:

```{python}
x = pd.DataFrame(data = {'a': [1, 1]})
x
x.duplicated()
x.drop_duplicates()
```
But now let's remove the only column:

```{python}
y = x.iloc[[0, 1], []]
y
y.duplicated()
y.drop_duplicates()
```

Like `data.table`, this treats the table as empty.

`duplicated` and `drop_duplicates` take a subset of columns to check: what if this subset is zero?

```{python}
z = pd.DataFrame(data = {'a': [1, 2]})
z
try: z.duplicated(subset = [])
except Exception as e: print(e)
try: z.drop_duplicates(subset = [])
except Exception as e: print(e)
```

Well, that's no good either.

## Why it's like this, and possible fixes

I can't say for the rest, but let's look at base R's code for `duplicated.data.frame`:

```{r}
duplicated.data.frame
```

Here we see an approach for looking for duplicate columns that I've used directly before: use `Map(list, x)`, after some tidying of `x`, to return a list of rows, where each row is given as the list of its values. We then check whether these rows are duplicated, so we're comparing list elements instead of several columns at once.

This is a reasonable approach if we have at least one column. What happens if we try this conversion with no columns?

Data frames are stored as a list, with each element giving a column's values, and the elements having to be the same length. If there are no columns, this list is empty:

```{r}
unclass(z)
```

Therefore, `Map(list, z)` returns an empty list, rather than a list of empty row lists:

```{r}
Map(list, z)
```

When we pass this into `duplicated`, of course, we get a zero-length logical vector.

I don't think there's much that can be done about this, outside of changing how data frames are stored. It's a strange situation where only the row names show that there are supposed to be 5 rows. If we make a copy where they're removed, as done in `data.table`, then this information is lost:

```{r}
a <- z
attr(a, "row.names") <- NULL # skips `row.names<-` sanity checks
a
unclass(a)
```

In turn, this information is only kept because, when asked for a data frame's row count, R uses the row names:

```{r}
nrow
dim.data.frame
```

Effectively, the row names are used like a "header", but for the rows instead of the columns. This is probably why, if you try to remove them with something like `row.names(a) <- NULL`, R will immediately add integer row names as replcaements: removing row names completely would break the information about the table's size, in a way that removing the column names can't. We can see this with tables that have columns, too:

```{r}
b <- data.frame(a = 1:4, b = 2:3)
dim(b)
attr(b, "row.names") <- NULL
dim(b) # R now thinks there are 0 rows...
unclass(b) # ... but the data's still there
length(b$a)
```

This means that we could fix `duplicated.data.frame` by having it make use of the row names. What would such an implementation of `duplicated` for tables look like? Writing it in a way that's agnostic to the number of columns is easy enough, but might be inefficient:

```{r}
duplicated2 <- function(x, incomparables = FALSE, fromLast = FALSE, ...) {
  UseMethod("duplicated2")
}
duplicated2.data.frame <- function(x, incomparables = FALSE, fromLast = FALSE, ...) {
  if (!isFALSE(incomparables)) 
    .NotYetUsed("incomparables != FALSE")
  if (any(i <- vapply(x, is.factor, NA))) 
    x[i] <- lapply(x[i], as.numeric)
  lst <- lapply(
    seq_along(row.names(x)),
    function(row) `names<-`(x[row, , drop = TRUE], NULL)
  )
  duplicated(lst, fromLast = fromLast, ...)
}
duplicated2(z)
duplicated2(w)
duplicated2(x)
duplicated2(y)
```

Maybe it's just better to add a second explicit edge case, so we're not relying on `nrow` using the row names:

```{r}
duplicated3 <- function(x, incomparables = FALSE, fromLast = FALSE, ...) {
  UseMethod("duplicated3")
}
duplicated3.data.frame <- function(x, incomparables = FALSE, fromLast = FALSE, ...) {
    if (!isFALSE(incomparables)) 
        .NotYetUsed("incomparables != FALSE")
    if (length(x) == 0L) {
      nr <- nrow(x)
      if (nr == 0L)
        return(logical())
      else
        return(c(FALSE, rep_len(TRUE, nr - 1L)))
    }
    if (length(x) != 1L) {
        if (any(i <- vapply(x, is.factor, NA))) 
            x[i] <- lapply(x[i], as.numeric)
        duplicated(do.call(Map, `names<-`(c(list, x), NULL)), 
            fromLast = fromLast)
    }
    else duplicated(x[[1L]], fromLast = fromLast, ...)}
duplicated3(z)
duplicated3(w)
duplicated3(x)
duplicated3(y)
```

It looks like it would also be quicker, at least for small tables like these:

```{r}
microbenchmark::microbenchmark(duplicated2(z), duplicated3(z), times = 1000)
microbenchmark::microbenchmark(duplicated2(w), duplicated3(w), times = 1000)
microbenchmark::microbenchmark(duplicated2(x), duplicated3(x), times = 1000)
microbenchmark::microbenchmark(duplicated2(y), duplicated3(y), times = 1000)
```

For `autodb` classes, I'll probably be writing something like the latter, so I don't have this edge case all over the code any more.

## Environment used

R session information:

```{r}
sessionInfo()
```

Python version (a little old, but installing/updating things in Python is so awful I don't want to do it again):

```{python}
import sys
print(sys.version)
```
