---
title: "Codex model"
output:
  html_document:
    number_sections: false
    self_contained: yes
    code_folding: none
    df_print: paged
author: "Mark Webster"
date: '`r format(Sys.time(), "Last modified: %d %b %Y")`'
---

```{r, "setup", include=FALSE}
library(data.table)
library(stringr)
library(DT)
library(widgetframe)
library(dplyr)
library(ggplot2)
library(rstan)
library(codex)
knitr::opts_chunk$set(echo = FALSE)
```

# Simulation configuration

```{r, "check ess"}
untidy_results <- readRDS("./model_files/vs_split.rds")
parm_info <- as.data.table(summary(untidy_results)$summary, keep.rownames = "parameter")[parameter != "lp_", 
                                                                                         c("parameter", "n_eff")]
```

Current model sample size is 4000. However, since we use a Markov Chain sampler, the samples are correlated: the effective sample size varies between different parameters, within the range (`r floor(min(parm_info$n_eff))`, `r ceiling(max(parm_info$n_eff))`).

# Posterior model performance

```{r, "read model results"}
results <- readRDS("./model_files/tidy_vs_split.rds")
```

```{r, "read model samples"}
sim <- results$tidy_results
```

```{r, "load data", message=FALSE}
normal_matches <- all_matches[recorder == "charnel_mouse" & (is.element(victory, c("normal", "")) | is.na(victory))]
mean_data <- prepare_match_data_for_modelling(normal_matches, starters, nicknames, mean = TRUE)
attach(mean_data)
starter_names <- starters
spec_names <- specs
starters <- codex::starters
```

```{r, "Matchup predictions for mean performance models"}
won_effect <- 2*rep(w, each = nrow(sim$matchup)) - 1
won <- w == 1
match_names <- paste0(normal_matches$tournament,
                      " Round ", normal_matches$round, 
                      ", Game ", normal_matches$round_match_number,
                      ": ", normal_matches$player1, " ", normal_matches$deck1,
                      " vs. ",
                      normal_matches$player2, " ", normal_matches$deck2, 
                      ", won by ", normal_matches$victor)
match_res <- colMeans(1/(1 + exp(-won_effect*sim$matchup[, 1:M, drop = FALSE])))
matchups_models <- data.table(`match name` = match_names,
                              `result probability` = match_res,
                              fairness = 1 - 2*abs(match_res - 1/2))
frameWidget(formatRound(datatable(matchups_models, rownames = FALSE, filter = "top",
                                  options = list(dom = "tip"),
                                  caption = "Recorded matches with model's expected probability of observed result. Fairness rates the fairness of the matchup, including players, between 0 and 1."), 
                        2:3, 2),
            height = "100%")
```

```{r, "Density plots for predictions for matches with simple decks"}
ggplot(matchups_models, aes(.data$`result probability`)) +
  geom_density() +
  xlab("result probability") +
  ggtitle("Post-hoc probability for given match results")
ggplot(matchups_models, aes(.data$`result probability`)) +
  geom_density() +
  xlab("result probability") +
  ggtitle("Post-hoc probability for given match results, by victor turn") +
  facet_wrap(. ~ factor(ifelse(won, "P1 win", "P2 win")))
matchups_models[, .(`result probability`, fairness,
                    P1_win_prob = ifelse(won, 
                                         `result probability`, 
                                         1 - `result probability`),
                    rounded = round(ifelse(won, 
                                           `result probability`, 
                                           1 - `result probability`),
                                    1),
                    win = won)
                ][, .(rate = ifelse(.N == 0, NA, mean(win)),
                      prob = ifelse(.N == 0, NA, mean(P1_win_prob))),
                  by = c("rounded")] %>% 
  ggplot(aes(.data$prob, .data$rate)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  xlab("mean forecast p1 win probability") + 
  ylab("observed p1 win rate") + 
  ggtitle("P1 win rate compared to predicted win probability grouped to nearest 10%")
```

If the model predicted matches by saying the most likely winner would win, its performance on the above matches, which were used to fit it, would be `r sum(matchups_models[["result probability"]] > 0.5)` out of `r nrow(matchups_models)` (`r round(mean(matchups_models[["result probability"]] > 0.5)*100, 1)`%). Since it gives a probability of each player winning, we can use proper scoring rules instead (the closer to zero, the better):

```{r, "model scores"}
model_log_scores <- matchups_models[, .(`log score` = -log(`result probability`), 
                                        `Brier score` = (1 - `result probability`)^2)
                                    ][, .(model = c("predict every match as 5-5", "current model"), 
                                          `log score` = c(log(2), mean(`log score`)), 
                                          `Brier score` = c(1/4, mean(`Brier score`)))]
frameWidget(formatRound(datatable(model_log_scores, options = list(dom = "t"), rownames = FALSE), 2, 3), height = "100%")
```

# Player skill

Player skills are given as their (additive) effect on their log-odds of winning a match. Skill is currently assumed to not change over time, so given skill levels for long-absent players are narrowly-distributed compared to how certain we'd really be about their current skill level. It'll also favour players who were veterans before the earliest recorded match, because the period where they learned the ropes is not included in their match records.

```{r, "extract vs split models player skills"}
vs_sim_skills <- melt(as.data.table(sim$player), id.vars = character(0), 
                      variable.name = "player", value.name = "player skill")[, .(`player skill`, mean = mean(`player skill`)), 
                                                                             by = "player"]
vs_split_player_levels <- vs_sim_skills[order(mean), unique(player)]

vs_skill <- vs_sim_skills[, .(player, `player skill`, mean, model = "Versus model")]
vs_skill_by_forum_skill <- vs_skill[, .(player = factor(player, vs_split_player_levels), `player skill`, mean, model)]
```

```{r, "plot vs models player skills", fig.height=10, warning=FALSE}
plot_player_skill(vs_skill_by_forum_skill[is.element(player, players)], "overall", waiver()) +
  ylim(-5, 5)
```

```{r, "plot probability player is best", fig.height=10}
prob_largest_skill <- get_probability_largest(sim$player)
plot_probability_largest(prob_largest_skill) +
  xlab("player") +
  ylab("probability best player")
```

```{r, "player skill table"}
frameWidget(formatRound(datatable(merge(vs_sim_skills[, .(mean = mean[1]), by = "player"
                                                      ][, .(player = as.character(player), `mean skill` = mean)],
                                        as.data.table(prob_largest_skill)[, .(player = effect, `prob. best (n = 4000)` = `effect size`)],
                                        on = "player", sort = FALSE),
                                  rownames = FALSE, filter = "top", options = list(dom = "tip")),
                        2, 3),
            height = "100%")
```

# Opposed component effects

Decks are treated in opposing pairs: P1 starter versus P2 starter, P1 starter versus each P2 spec, and so on. Each match has 16 such pairs. Each pair's effect is given as its additive effect on the log-odds of a player 1 victory. The component's effects are added to given overall matchup between the decks, before accounting for player skill levels.

Note that these pair effects are *not* direct appraisals of how the components fare against each other. For each, the Green vs. Black effect doesn't assess how those two decks decks match up against each other, it assesses how decks using those starter decks *tend* to match up against each other. Similarly, Blood vs. Future doesn't, directly assess how those two specs compete at, say, Tech II, because I don't record tech building choices. Instead, it shows how P1 decks including Blood tend to fare against P2 decks including Future. Note that this also ignores interactions between different pairs completely.

To examine the matchup between two particular decks, add their components in the relevant Deck components column. The overall matchup is then given below the table, as both the log-odds and the probability of a player 1 victory. Individual pair effects are given in the displayed table rows.

Currently I've not added the players in the same table to account for skill effects. In the meantime, since player skill tends to have a larger effect than the deck matchup, don't compare the deck matchups to your own match outcomes too strictly, unless you can manually add the effects from the player table (remember to subtract the P2 effect, not add it).

```{r, "extract vs model matrices"}
vs_array <- extract_vs_model_array(results)
```

```{r, "simple versus table"}
vs_split_table <- as.data.table(vs_array)[, .(sample = V1, 
                                              `P1 component` = factor(V2, c(starter_names, spec_names)),
                                              `P2 component` = factor(V3, c(starter_names, spec_names)),
                                              value)
                                          ][, .(mean = mean(value)),
                                            by = c("P1 component", "P2 component")
                                            ][order(`P1 component`, `P2 component`)]
vs_sketch <- htmltools::withTags(table(class = "display",
                                       thead(tr(th(colspan = 2, "Deck components"),
                                                th(rowspan = 2, "Mean Player 1 win log-odds effect")),
                                             tr(lapply(c("Player 1", "Player 2"), th))),
                                       tfoot(tr(th("Player 1"), th("Player 2"), th(0)))))
vs_split_JS_code <- "function(row, data, start, end, display) {
  var api = this.api(), data;
  total_mean = api.column(2, {search: 'applied'}).data().reduce( function(a, b) { return a + b}, 0);
  $( api.column(2).footer() ).html('Total: ' + total_mean.toFixed(2) + '; P1 win prob: ' + (100/(1+Math.exp(-total_mean))).toFixed(1) + '%')
  }"
frameWidget(datatable(vs_split_table, rownames = FALSE, filter = "top", container = vs_sketch, 
                      options = list(dom = "ftip", pageLength = 16, footerCallback = JS(vs_split_JS_code))) %>% 
              formatRound(3, digits = 3),
            height = "100%")
```

# Monocolour matchups

Since we're most interested in whether monocolour decks are reasonably balanced, here are matchup results for the monocolour decks. The three black vertical lines in each plot facet show the matchup quartiles.

```{r, "extract monodecks"}
mononames <- sort(paste0("Mono", setdiff(starters$starter, "Neutral")))
monobits <- prepare_deck_names_for_modelling(mononames, starters, nicknames)
```

```{r, "extract vs split model monocolour matchups"}
vs_monos <- get_matchups(results, mononames)
```

```{r, make vs model monocolour matchup plots}
plot_matchup_samples(vs_monos)
```

```{r, "print vs model monocolour matchups in order"}
vs_monos_print <- vs_monos[, .(`P1 win probability` = mean(prob_matchup)), 
                           by = c("P1", "P2")
                           ][, .(P1, P2, `P1 win probability`,
                                 matchup = paste(format(round(`P1 win probability`, 2)*10, nsmall = 1),
                                                 format(10 - round(`P1 win probability`, 2)*10, nsmall = 1),
                                                 sep = "-"),
                                 fairness = 1 - 2*abs(`P1 win probability` - 1/2))]
frameWidget(formatRound(datatable(vs_monos_print, rownames = FALSE, filter = "top",
                                  options = list(dom = "ftip")), 
                        c(3, 5), c(3, 2)),
            height = "100%")
```

# Model variances

Each type of component in the model has a different variance in the effect; inference for the variances is also done in the model simulation. The below plot shows the variances for each component type, scaled by how many such components go into a matchup, i.e. two player skill components, one starter vs. starter component, six starter vs. spec / spec vs. starter components, and nine spec vs. spec components.

```{r, "plot versus model variances"}
variance_info <- get_variances(sim)
plot_variances(variance_info)
```

On average, total player skill effects on match outcome are about `r round(variance_info[type == "player/deck ratio", mean(value)], 2)` as variable as total deck effects. This is a rough measure of how important to a match the players are, compared to the decks.

# Nash equilibria

For a set of decks, we can take the matchup samples, and determine a Nash equilibrium for choosing which deck to use from that set. This can be done in two ways:

- Greedy: Find the Nash equilibrium of the mean matchups. This picks a strategy based on taking the model's predicted matchups as correct. Using it to choose a deck is equivalent to always choosing the arm currently considered most likely to be the best in a multi-armed bandit problem. For the above plot of each player's probability of being the best player, this would always state that the most-likely-best player is the best.
- Calibrated: Find the Nash equilibria for each sample of matchups, and take the mean equilibrium. This takes proper account of the model's uncertainty over the matchups. Using it to choose a deck is equivalent to Thompson sampling: for player skills, this would randomly choose a player to state as the best, weighted by the players' probabilities of being the best. This approach is better calibrated.

## Monocolour

Greedy:

```{r, "versus model equilibrium"}
mono_matrix <- as.matrix(dcast(unique(vs_monos[, c("P1", "P2", "prob_mean")]),
                               P1 ~ P2, value.var = "prob_mean"), 
                         rownames = "P1")
frameWidget(formatRound(datatable(get_nash_equilibria(mono_matrix), options = list(dom = "t"), rownames = FALSE), 
                        2:8, 3), 
            height = "100%")
```

Calibrated:

```{r, "probabilistic versus model equilibrium"}
vs_mono_array <- get_matchup_array(results, mononames)
mono_stoch_nash <- get_nash_equilibria(vs_mono_array)
mono_mean_nash <- apply(mono_stoch_nash, 1:2, mean)
frameWidget(formatRound(datatable(as.data.table(mono_mean_nash, keep.rownames = "Player"), options = list(dom = "t"),
                                  rownames = FALSE), 
                        2:8, 3), 
            height = "100%")
```

As would be expected, the chance of winning before determining play order is 50%. Black is dominant, as expected given that it's known as a strong faction. The only exception is when known to be going first, where Red plays a significant role. However, this is less clear-cut in the calibrated results: even Blue, considered weak due to an especially lopsided matchup against Black, hasn't been completely cut out.

## All recorded decks

```{r, "construct all possible mono/multicolour/draftable decks"}
possible_spec_trios <- utils::combn(codex::starters$spec, 3L, paste, collapse = "/")
possible_decks <- expand.grid(possible_spec_trios, unique(codex::starters$starter)) %>% 
  apply(1, paste, collapse = "/") %>% 
  standardise_deck_name(codex::starters)
possible_normal_decks <- possible_decks[str_count(possible_decks, "/") == 2L]
possible_deck_names <- standardise_deck_name(possible_decks, codex::starters, codex::nicknames)
possible_normal_deck_names <- standardise_deck_name(possible_normal_decks, codex::starters, codex::nicknames)

possible_components <- prepare_deck_names_for_modelling(possible_deck_names,
                                                        codex::starters, codex::nicknames)
possible_normal_components <- prepare_deck_names_for_modelling(possible_normal_deck_names,
                                                               codex::starters, codex::nicknames)
# standard decks would take about 30 hours with the current function, so here's a smaller case.
used_decks <- unique(c(matches$deck1[!is.na(matches$deck1)],
                       matches$deck2[!is.na(matches$deck2)]))
used_normal_decks <- used_decks[str_count(used_decks, "/") != 3]
test_samples <- get_matchup_array(results, used_normal_decks)
test_matrix <- apply(test_samples, 2:3, mean)
```

The current way I calculate Nash equilbria makes computing them for all `r length(possible_normal_deck_names)` possible standard multicolour decks infeasible. I'm working on fixing this. In the meantime, here are the Nash equilibria for all `r length(used_normal_decks)` such decks used in a recorded tournament match.

Nash equilibria are given for three types of player: `P1` when known to be going first, `P2` when known to be going second, and `Both` for the more common case where the deck is chosen before a coin flip to determine who goes first.

```{r, "get used normal deck Nash equilibria"}
test_nash <- get_nash_equilibria(test_matrix)
test_stoch_nash <- readRDS("./model_files/nash_samples_vs_split.rds")
test_stoch_mean_nash <- as.data.table(apply(test_stoch_nash, 1:2, mean), 
                                      keep.rownames = "Player")[, c(.(Player = factor(Player, 
                                                                                      c("P1", "P2", "Both"))), 
                                                                    .SD), 
                                                                .SDcols = dimnames(test_stoch_nash)[[2]]]
```

### Win probability

Greedy:

```{r, "greedy possibly normal Nash datatable win probabilities"}
frameWidget(formatRound(datatable(print_nash(test_nash), options = list(dom = "t"), rownames = FALSE), 2, 3), height = "100%")
```

Calibrated:

```{r, "calibrated possibly normal Nash datatable win probabilities"}
frameWidget(formatRound(datatable(print_nash(test_stoch_mean_nash), options = list(dom = "t"), rownames = FALSE), 2, 3), 
            height = "100%")
```

The first player seems to be more advantaged than in the monocolour game, though not by much.

### Non-zero deck weights

Greedy:

```{r, "greedy possibly normal Nash datatable"}
frameWidget(DT_nash(test_nash, options = list(dom = "tip")),
            height = "100%")
```

Calibrated:

```{r, "calibrated possibly normal Nash datatable"}
frameWidget(DT_nash(test_stoch_mean_nash, round = 4, options = list(dom = "tip")),
            height = "100%")
```

`r if(ncol(test_stoch_mean_nash) == 2L + length(used_normal_decks)) "Notably, no decks have been eliminated in the calibrated version, so there is still a lot of uncertainty."`
